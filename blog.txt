ref: https://zhuanlan.zhihu.com/p/684120282  GPT-SOVITS 技术原理

推理过程：
输入：
    待合成文本   text
    参考文本    ref_text（phones1）
    参考音频    ref_wav（phones2）

文本处理：
    1、text_ids转换为发音phone_ids
    2、根据phone_ids编码bert特征
    3、取bert模型倒数第2层tensor，同时去掉SOS和EOS

音频处理：
    1、实用cn_hubert,基于wav逐层encode，抽取语音hubert自监督向量
    2、过codebook，得到wav对应的codebook ids
    3、基于wav，收取频谱spec帧特征

过vall-e推理（Text2SemanticDecoder）
    1、实用phone_ids、phone_bert特征，参考音频的wav codebook ids
    2、预测pred_semantic和idx，idx用于截断pred_semantic（去掉参考音频的特征）

过vits模型decode推理（SynthesizerTrn）
    1、使用vall-e预测的pred_semantic,phones2，参考音频的语音帧refer_spec
    2、vits decode解码，直接得到对应的wav

vall-e模型推理


Text2SemanticLightningModule(
  (model): Text2SemanticDecoder(
    (bert_proj): Linear(in_features=1024, out_features=512, bias=True)
    (ar_text_embedding): TokenEmbedding(
      (dropout): Dropout(p=0, inplace=False)
      (word_embeddings): Embedding(512, 512)
    )
    (ar_text_position): SinePositionalEmbedding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (ar_audio_embedding): TokenEmbedding(
      (dropout): Dropout(p=0, inplace=False)
      (word_embeddings): Embedding(1025, 512)
    )
    (ar_audio_position): SinePositionalEmbedding(
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (h): TransformerEncoder(
      (layers): ModuleList(
        (0-23): 24 x TransformerEncoderLayer(
          (self_attn): MultiheadAttention(
            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
          )
          (linear1): Linear(in_features=512, out_features=2048, bias=True)
          (dropout): Dropout(p=0.1, inplace=False)
          (linear2): Linear(in_features=2048, out_features=512, bias=True)
          (dropout1): Dropout(p=0.1, inplace=False)
          (dropout2): Dropout(p=0.1, inplace=False)
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
        )
      )
    )
    (ar_predict_layer): Linear(in_features=512, out_features=1025, bias=False)
    (loss_fct): CrossEntropyLoss()
    (ar_accuracy_metric): MulticlassAccuracy()
  )
)


vits模型推理解码

    1、使用vall-e预测的pred_semantic,phones2，参考音频的语音帧refer_spec
    2、vits decode解码，直接得到对应的wav

涉及到的模块
    1、code量化解码模块，使用ResidualVectorQuantizer，将vall-e预测的pred_semantic code_ids进行量化解码，
        例如输入shape[1,129] -> 输出shape [1,768,129]
    2、ref_enc-语音mel帧编码模块，使用MelStyleEncoder
    SynthesizerTrn(
  (enc_p): TextEncoder(
    (ssl_proj): Conv1d(768, 192, kernel_size=(1,), stride=(1,))
    (encoder_ssl): Encoder(
      (drop): Dropout(p=0.1, inplace=False)
      (attn_layers): ModuleList(
        (0-2): 3 x MultiHeadAttention(
          (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_1): ModuleList(
        (0-2): 3 x LayerNorm()
      )
      (ffn_layers): ModuleList(
        (0-2): 3 x FFN(
          (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))
          (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_2): ModuleList(
        (0-2): 3 x LayerNorm()
      )
    )
    (encoder_text): Encoder(
      (drop): Dropout(p=0.1, inplace=False)
      (attn_layers): ModuleList(
        (0-5): 6 x MultiHeadAttention(
          (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_1): ModuleList(
        (0-5): 6 x LayerNorm()
      )
      (ffn_layers): ModuleList(
        (0-5): 6 x FFN(
          (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))
          (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_2): ModuleList(
        (0-5): 6 x LayerNorm()
      )
    )
    (text_embedding): Embedding(322, 192)
    (mrte): MRTE(
      (cross_attention): MultiHeadAttention(
        (conv_q): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv_k): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv_v): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (conv_o): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
        (drop): Dropout(p=0.0, inplace=False)
      )
      (c_pre): Conv1d(192, 512, kernel_size=(1,), stride=(1,))
      (text_pre): Conv1d(192, 512, kernel_size=(1,), stride=(1,))
      (c_post): Conv1d(512, 192, kernel_size=(1,), stride=(1,))
    )
    (encoder2): Encoder(
      (drop): Dropout(p=0.1, inplace=False)
      (attn_layers): ModuleList(
        (0-2): 3 x MultiHeadAttention(
          (conv_q): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_k): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_v): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (conv_o): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_1): ModuleList(
        (0-2): 3 x LayerNorm()
      )
      (ffn_layers): ModuleList(
        (0-2): 3 x FFN(
          (conv_1): Conv1d(192, 768, kernel_size=(3,), stride=(1,))
          (conv_2): Conv1d(768, 192, kernel_size=(3,), stride=(1,))
          (drop): Dropout(p=0.1, inplace=False)
        )
      )
      (norm_layers_2): ModuleList(
        (0-2): 3 x LayerNorm()
      )
    )
    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))
  )
  (dec): Generator(
    (conv_pre): Conv1d(192, 512, kernel_size=(7,), stride=(1,), padding=(3,))
    (ups): ModuleList(
      (0): ConvTranspose1d(512, 256, kernel_size=(16,), stride=(10,), padding=(3,))
      (1): ConvTranspose1d(256, 128, kernel_size=(16,), stride=(8,), padding=(4,))
      (2): ConvTranspose1d(128, 64, kernel_size=(8,), stride=(2,), padding=(3,))
      (3): ConvTranspose1d(64, 32, kernel_size=(2,), stride=(2,))
      (4): ConvTranspose1d(32, 16, kernel_size=(2,), stride=(2,))
    )
    (resblocks): ModuleList(
      (0): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (1): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (2): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(256, 256, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (3): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (4): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (5): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(128, 128, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (6): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (7): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (8): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(64, 64, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (9): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (10): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (11): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(32, 32, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
      (12): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))
          (1): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(3,), dilation=(3,))
          (2): Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(5,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(16, 16, kernel_size=(3,), stride=(1,), padding=(1,))
        )
      )
      (13): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))
          (1): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(9,), dilation=(3,))
          (2): Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(15,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(16, 16, kernel_size=(7,), stride=(1,), padding=(3,))
        )
      )
      (14): ResBlock1(
        (convs1): ModuleList(
          (0): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))
          (1): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(15,), dilation=(3,))
          (2): Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(25,), dilation=(5,))
        )
        (convs2): ModuleList(
          (0-2): 3 x Conv1d(16, 16, kernel_size=(11,), stride=(1,), padding=(5,))
        )
      )
    )
    (conv_post): Conv1d(16, 1, kernel_size=(7,), stride=(1,), padding=(3,), bias=False)
    (cond): Conv1d(512, 512, kernel_size=(1,), stride=(1,))
  )
  (enc_q): PosteriorEncoder(
    (pre): Conv1d(1025, 192, kernel_size=(1,), stride=(1,))
    (enc): WN(
      (in_layers): ModuleList(
        (0-15): 16 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
      )
      (res_skip_layers): ModuleList(
        (0-14): 15 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
        (15): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
      )
      (drop): Dropout(p=0, inplace=False)
      (cond_layer): Conv1d(512, 6144, kernel_size=(1,), stride=(1,))
    )
    (proj): Conv1d(192, 384, kernel_size=(1,), stride=(1,))
  )
  (flow): ResidualCouplingBlock(
    (flows): ModuleList(
      (0): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (1): Flip()
      (2): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (3): Flip()
      (4): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (5): Flip()
      (6): ResidualCouplingLayer(
        (pre): Conv1d(96, 192, kernel_size=(1,), stride=(1,))
        (enc): WN(
          (in_layers): ModuleList(
            (0-3): 4 x Conv1d(192, 384, kernel_size=(5,), stride=(1,), padding=(2,))
          )
          (res_skip_layers): ModuleList(
            (0-2): 3 x Conv1d(192, 384, kernel_size=(1,), stride=(1,))
            (3): Conv1d(192, 192, kernel_size=(1,), stride=(1,))
          )
          (drop): Dropout(p=0, inplace=False)
          (cond_layer): Conv1d(512, 1536, kernel_size=(1,), stride=(1,))
        )
        (post): Conv1d(192, 96, kernel_size=(1,), stride=(1,))
      )
      (7): Flip()
    )
  )
  (ref_enc): MelStyleEncoder(
    (spectral): Sequential(
      (0): LinearNorm(
        (fc): Linear(in_features=1025, out_features=128, bias=True)
      )
      (1): Mish()
      (2): Dropout(p=0.1, inplace=False)
      (3): LinearNorm(
        (fc): Linear(in_features=128, out_features=128, bias=True)
      )
      (4): Mish()
      (5): Dropout(p=0.1, inplace=False)
    )
    (temporal): Sequential(
      (0): Conv1dGLU(
        (conv1): ConvNorm(
          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (1): Conv1dGLU(
        (conv1): ConvNorm(
          (conv): Conv1d(128, 256, kernel_size=(5,), stride=(1,), padding=(2,))
        )
        (dropout): Dropout(p=0.1, inplace=False)
      )
    )
    (slf_attn): MultiHeadAttention(
      (w_qs): Linear(in_features=128, out_features=128, bias=True)
      (w_ks): Linear(in_features=128, out_features=128, bias=True)
      (w_vs): Linear(in_features=128, out_features=128, bias=True)
      (attention): ScaledDotProductAttention(
        (softmax): Softmax(dim=2)
        (dropout): Dropout(p=0.1, inplace=False)
      )
      (fc): Linear(in_features=128, out_features=128, bias=True)
      (dropout): Dropout(p=0.1, inplace=False)
    )
    (fc): LinearNorm(
      (fc): Linear(in_features=128, out_features=512, bias=True)
    )
  )
  (ssl_proj): Conv1d(768, 768, kernel_size=(2,), stride=(2,))
  (quantizer): ResidualVectorQuantizer(
    (vq): ResidualVectorQuantization(
      (layers): ModuleList(
        (0): VectorQuantization(
          (project_in): Identity()
          (project_out): Identity()
          (_codebook): EuclideanCodebook()
        )
      )
    )
  )
)